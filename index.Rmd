---
title: "Practical Machine Learning - Course Project"
output: html_document
---


##Purpose

The goal of this project is to build a model from the Weight Lifting Exercise Dataset available at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv ([website](http://groupware.les.inf.puc-rio.br/har )), evaluate the accuracy, explain the method and decisions made, and generate predictions for a testing set https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv.

##Summary

In this assignment the modeling effort consisted of data preparation, selecting factors, partitioning the training data, building the model, and evaluating the result. The resulting model uses a random forest algorithm trained against a partitioned data set with a 75%/25% split for traing/validation. Its out of sample error is 0.69%.

##data preparation

The training set, at nearly 20k rows, seems large enough to comfortably create a validation set. I chose random subsampling without replacement to evenly distribute test data across all participants

Data cleanup achieved the following:

*remove unwanted factors* Factors that qualitatively aren't correlated with a generalizable prediction algorithm, such as indicies, timestamps, windows, and names, are specific to the training activity and don't add information to a generalizable model that can be used outside of training.

*remove noisy factors*. Primarily factors with high percentages of invalid to valid data (NA or Div/0) would prevent an algorithm from successful training. Factors with a greater than .5 ratio were removed from the training set.

*convert data types*. Data as loaded from .csv required some configuration to avoid continuous data from being converted to factors, and categorical types (such as classe) were explicitly converted to factors to support the modeling algorithm.


##Algorithm 

I used a random forest to develop the model chosen for its strength in classification and mitigation of bias and overfitting, which is useful for developing a generalized model, which is a concern when only sampling data from 6 training subjects. In another alternative attempt using _generalized linear model_ algorithm, the algorithm was unable to establish predictors from the dataset.

#Training

I ran traditional cross-validation on the samples withheld from training to evaluate the effectiveness of the models. The random forest model yielded 99.8% accuracy with 27 parameters chosen from among 52 factors on a set of 14718 samples. Bootstrapping was done with 25 reps and sample sizes of 14718

```{r modelDetail, echo=FALSE, message=F, warning=F}
source("assignment.R")
set.seed(1242)
modelFile <- 'modelRF_75train_notimestamps_50noise.Rds'
data <- cleanData(noiseTolerance = 0.5
                  , trainingRatio = 0.75
                  , exclusionSet = c("timestamp"
                                     #,"kurtosis"
                                     #,"skewness" 
                  )
                  , exclusions = c("X", 
                                   "user_name", #not sensor data
                                   "new_window", #not sensor data
                                   "num_window", #not sensor data
                                   "amplitude_yaw_belt", #garbage data
                                   "amplitude_yaw_dumbbell", #garbage data
                                   "amplitude_yaw_forearm",#garbage data
                                   "cvtd_timestamp", #not correlated
                                   "skewness_yaw_forearm", #garbage data
                                   "kurtosis_yaw_forearm", #garbage data
                                   "skewness_yaw_dumbbell", #garbage data
                                   "kurtosis_yaw_dumbbell", #garbage data
                                   "skewness_yaw_belt", #garbage data
                                   "kurtosis_yaw_belt") #garbage data
                  )
result <- generateModel(modelRds = modelFile
                       , training = data$training
                       , testing = data$testing )
                       
print(result$model)
```

#Out of Sample Error

The random forest algorithm averages the out of bag error at each branch, and yields its own estimate of out of sample error from the training set, estimated to be 0.69%.

```{r}
print(result$model$finalModel)
```


After training, I performed a separate, conventional validation, demonstrating that the model has a 99.8% accuracy rate on the training data held out of the training set. 

```{r matrix, echo=FALSE}
result$confusionMatrix
```

This error rate could be reduced perfect prediction by including the window data, but I considered it to be a sign of overfitting. The predictive model run against the testing set yields the following predictions

```{r prediction, echo=FALSE}
prediction <- predict(result$model, dataTestingPml)
print(prediction)
```

##conclusions

The random forest model has shown to be predict accurately against the validation set and through cross validation. The test set draws from the same set of data, and I expect it will predict accurately against it. However, the model is tuned to a sample from six subjects and its unclear how well this model will generalize to other subjects.


